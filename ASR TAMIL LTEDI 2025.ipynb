{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11196940,"sourceType":"datasetVersion","datasetId":6990591},{"sourceId":11787624,"sourceType":"datasetVersion","datasetId":7401242}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!add-apt-repository -y ppa:jonathonf/ffmpeg-4\n!apt update\n!apt install -y ffmpeg\n!pip install datasets>=2.6.1\n!pip install git+https://github.com/huggingface/transformers\n!pip install librosa\n!pip install evaluate>=0.30\n!pip install jiwer\n!pip install gradio\n!pip install -q bitsandbytes datasets accelerate loralib\n!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git@main","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 0: Setup in Kaggle Notebook librosa datasets transformers evaluate\n!pip install torchaudio  soundfile gdown pandas chardet num2words jiwer tqdm \nimport gdown\nimport os\nimport pandas as pd\nimport torchaudio\nfrom datasets import Dataset, DatasetDict\nimport librosa\nimport soundfile as sf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# #modified for testing purpose ","metadata":{}},{"cell_type":"code","source":"def check_dataset(audio_dir, text_dir):\n    audio_files = set([f.split('.')[0] for f in os.listdir(audio_dir) if f.endswith('.wav')])\n    text_files = set([f.split('.')[0] for f in os.listdir(text_dir) if f.endswith('.txt')])\n    \n    missing_audio = text_files - audio_files\n    missing_text = audio_files - text_files\n    \n    print(f\"Total pairs: {len(audio_files & text_files)}\")\n    print(f\"Missing audio for {len(missing_audio)} text files\")\n    print(f\"Missing text for {len(missing_text)} audio files\")\n    \ncheck_dataset('/kaggle/input/tamil-dataset-for-asr/Training/Audio', '/kaggle/input/tamil-dataset-for-asr/Training/Transcripts')\n\nall_files = [f.split('.')[0] for f in os.listdir('/kaggle/input/tamil-dataset-for-asr/Training/Audio') if f.endswith('.wav')]\ndf = pd.DataFrame({'file_id': all_files})\n\n# Kaggle: Use fixed random_state for reproducibility\n# First split: take 5 samples for final test\nfinal_test_df = df.sample(n=5, random_state=42)\nremaining_df = df.drop(final_test_df.index)\n\n# Then split the remaining data into train and validation\ntrain_df = remaining_df.sample(frac=0.99, random_state=42)  # Using 99% of remaining for training\ntest_df = remaining_df.drop(train_df.index)  # The rest for validation\n\n# Save splits\ntrain_df.to_csv('/kaggle/working/train_files.csv', index=False)\ntest_df.to_csv('/kaggle/working/val_files.csv', index=False)  # Validation set\nfinal_test_df.to_csv('/kaggle/working/final_test_files.csv', index=False)  # Final test set\n\nprint(f\"Training samples: {len(train_df)}\")\nprint(f\"Validation samples: {len(test_df)}\")\nprint(f\"Final test samples: {len(final_test_df)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_test_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyloudnorm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pyloudnorm\n\nfrom pyloudnorm import Meter\n\n# def preprocess_audio(audio_path):\n#     waveform, sr = librosa.load(audio_path, sr=16000, mono=True)\n#     meter = pyloudnorm.Meter(sr) \n#     loudness = meter.integrated_loudness(waveform)\n#     target_loudness = -16  # Optimal for speech\n#     waveform = pyloudnorm.normalize.loudness(waveform, loudness, target_loudness)\n#     waveform = waveform - np.mean(waveform)\n#     waveform = np.clip(waveform, -1.0, 1.0)\n#     output_path = f\"/kaggle/working/processed_audio/{os.path.basename(audio_path)}\"\n#     sf.write(output_path, waveform, 16000, subtype='PCM_16')\n#     return output_path.strip\n\n\n\ndef preprocess_audio(audio_path):\n    waveform, sr = librosa.load(audio_path, sr=16000, mono=True)\n    \n    # Silence removal with buffers and duration check\n    intervals = librosa.effects.split(waveform, top_db=40, frame_length=1024, hop_length=256)\n    \n    # Add 200ms buffers and filter short segments\n    buffer_samples = int(0.2 * sr)\n    min_samples = int(0.3 * sr)\n    \n    processed_intervals = []\n    for start, end in intervals:\n        buffered_start = max(0, start - buffer_samples)\n        buffered_end = min(len(waveform), end + buffer_samples)\n        if (buffered_end - buffered_start) > min_samples:\n            processed_intervals.append((buffered_start, buffered_end))\n    \n    if not processed_intervals:\n        print(f\"No speech found in {audio_path}\")\n        return None\n        \n    non_silent_audio = np.concatenate(\n        [waveform[start:end] for start, end in processed_intervals]\n    )\n    \n    # Post-processing\n    non_silent_audio -= np.mean(non_silent_audio)  # DC offset\n    non_silent_audio = np.clip(non_silent_audio, -1.0, 1.0)\n    \n    # Peak normalization (better than loudness norm for ASR)\n    peak = np.max(np.abs(non_silent_audio))\n    if peak > 0:\n        non_silent_audio /= peak\n        \n    # Save output\n    os.makedirs(\"/kaggle/working/processed_audio\", exist_ok=True)\n    output_path = f\"/kaggle/working/processed_audio/{os.path.basename(audio_path)}\"\n    sf.write(output_path, non_silent_audio, sr, subtype=\"PCM_16\")\n    \n    return output_path\n    \n# def process_all_audio(df):\n#     os.makedirs('/kaggle/working/processed_audio', exist_ok=True)\n#     df['audio_path'] = df['file_id'].apply(\n#         lambda x: preprocess_audio(f\"/kaggle/input/tamil-dataset-for-asr/Training/Audio/{x}.wav\")\n#     )\n#     return df\n\ndef process_all_audio(df):\n    # Skip creating a processed_audio directory (no preprocessing needed)\n    # Just assign the original file paths directly\n    df['audio_path'] = df['file_id'].apply(\n        lambda x: f\"/kaggle/input/tamil-dataset-for-asr/Training/Audio/{x}.wav\"\n    )\n    return df\n\ntrain_df = process_all_audio(train_df)\ntest_df = process_all_audio(test_df)\nfinal_test_df=process_all_audio(final_test_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train columns:\", train_df.columns.tolist())\nprint(\"Test columns:\", test_df.columns.tolist())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['audio_path'][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport unicodedata\n\ndef normalize_tamil_text(text):\n    text = re.sub(r'<\\|.*?\\|>', '', text)\n    text = text.replace('\\u200c', '')\n    text = unicodedata.normalize('NFKC', text)\n    tamil_punctuation = '''!(),–‘“”…•॥ॐ।<>?@[\\\\]^_`{|}~'''\n    translator = str.maketrans('', '', tamil_punctuation)\n    text = text.translate(translator)\n    return text.strip()\n\ndef load_transcript(file_id):\n    txt_path = f\"/kaggle/input/tamil-dataset-for-asr/Training/Transcripts/{file_id}.txt\"\n    encodings = ['utf-8', 'utf-16', 'utf-16-le', 'utf-16-be', 'latin1']\n    \n    for encoding in encodings:\n        try:\n            with open(txt_path, 'r', encoding=encoding) as f:\n                text = f.read().strip()\n                \n            if any('\\u0B80' <= char <= '\\u0BFF' for char in text[:10]):\n                text = normalize_tamil_text(text)\n                return text\n        except UnicodeError:\n            continue\n    print(f\"Failed to decode {file_id}.txt with common encodings\")\n    return None\n\ntrain_df['text'] = train_df['file_id'].apply(load_transcript)\ntest_df['text'] = test_df['file_id'].apply(load_transcript)\nfinal_test_df['text'] = final_test_df['file_id'].apply(load_transcript)\ntrain_df = train_df.dropna(subset=['text'])\ntest_df = test_df.dropna(subset=['text'])\nfinal_test_df = final_test_df.dropna(subset=['text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport librosa\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n\n# 1. Basic Dataset Statistics\ndef print_basic_stats(df, name):\n    total_duration = df['duration'].sum()\n    avg_duration = df['duration'].mean()\n    avg_text_len = df['text_length'].mean()\n    \n    print(f\"\\n{name} Set:\")\n    print(f\"Samples: {len(df):,}\")\n    print(f\"Total Duration: {total_duration/3600:.2f} hours\")\n    print(f\"Avg Duration: {avg_duration:.2f}s\")\n    print(f\"Avg Text Length: {avg_text_len:.0f} characters\")\n\n# Compute durations and text lengths\ndef compute_metrics(df):\n    df['duration'] = df['audio_path'].apply(lambda x: librosa.get_duration(path=x))\n    df['text_length'] = df['text'].str.len()\n    return df\n\ntrain_df = compute_metrics(train_df)\ntest_df = compute_metrics(test_df)\nfinal_test_df = compute_metrics(final_test_df)\n\n# Print basic statistics\nprint(\"=== Dataset Statistics ===\")\nprint_basic_stats(train_df, \"Training\")\nprint_basic_stats(test_df, \"Validation\")\nprint_basic_stats(final_test_df, \"Final Test\")\n\n# 2. Sample Distribution Visualization\nplt.figure(figsize=(10, 5))\nsns.barplot(x=['Training', 'Validation', 'Final Test'],\n            y=[len(train_df), len(test_df), len(final_test_df)])\nplt.title('Sample Distribution Across Splits')\nplt.ylabel('Number of Samples')\nplt.show()\n\n# 3. Audio Duration Distribution\nplt.figure(figsize=(12, 6))\nsns.histplot(train_df['duration'], bins=50, color='blue', alpha=0.5, label='Train')\nsns.histplot(test_df['duration'], bins=50, color='orange', alpha=0.5, label='Validation')\nsns.histplot(final_test_df['duration'], bins=50, color='green', alpha=0.5, label='Final Test')\nplt.title('Audio Duration Distribution')\nplt.xlabel('Duration (seconds)')\nplt.ylabel('Count')\nplt.legend()\nplt.show()\n\n# 4. Text Length Distribution\nplt.figure(figsize=(12, 6))\nsns.histplot(train_df['text_length'], bins=50, color='blue', alpha=0.5, label='Train')\nsns.histplot(test_df['text_length'], bins=50, color='orange', alpha=0.5, label='Validation')\nplt.title('Text Length Distribution (Characters)')\nplt.xlabel('Text Length (characters)')\nplt.ylabel('Count')\nplt.legend()\nplt.show()\n\n# 5. Character Frequency Analysis\nall_text = pd.concat([train_df['text'], test_df['text'], final_test_df['text']]).str.cat()\nchar_counts = Counter(all_text)\ntop_chars = dict(sorted(char_counts.items(), key=lambda x: x[1], reverse=True)[:20])\n\nplt.figure(figsize=(15, 5))\nsns.barplot(x=list(top_chars.keys()), y=list(top_chars.values()))\nplt.title('Top 20 Most Frequent Characters')\nplt.xlabel('Character')\nplt.ylabel('Count')\nplt.show()\n\n# 6. Data Leakage Check\ntrain_texts = set(train_df['text'])\ntest_texts = set(test_df['text'])\nfinal_test_texts = set(final_test_df['text'])\n\noverlap_train_test = train_texts & test_texts\noverlap_train_final = train_texts & final_test_texts\noverlap_test_final = test_texts & final_test_texts\n\nprint(\"\\n=== Data Leakage Check ===\")\nprint(f\"Common texts between Train-Validation: {len(overlap_train_test)}\")\nprint(f\"Common texts between Train-Final Test: {len(overlap_train_final)}\")\nprint(f\"Common texts between Validation-Final Test: {len(overlap_test_final)}\")\n\n# 7. Audio Waveform Examples (for 3 random samples)\ndef plot_waveforms(df, title):\n    plt.figure(figsize=(15, 3))\n    for i in range(3):\n        idx = np.random.choice(df.index)\n        audio_path = df.loc[idx, 'audio_path']\n        waveform, sr = librosa.load(audio_path, sr=16000)\n        plt.subplot(1, 3, i+1)\n        librosa.display.waveshow(waveform, sr=sr)\n        plt.title(f\"Sample {idx}\\nDuration: {df.loc[idx, 'duration']:.2f}s\")\n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()\n\nplot_waveforms(train_df, \"Training Set Waveform Examples\")\nplot_waveforms(test_df, \"Validation Set Waveform Examples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_spectrograms(df, title, sr=16000):\n    plt.figure(figsize=(15, 10))\n    for i in range(3):\n        idx = np.random.choice(df.index)\n        audio_path = df.loc[idx, 'audio_path']\n        y, sr = librosa.load(audio_path, sr=sr)\n        \n        plt.subplot(3, 2, 2*i+1)\n        librosa.display.waveshow(y, sr=sr)\n        plt.title(f\"Waveform - Sample {idx}\")\n        \n        plt.subplot(3, 2, 2*i+2)\n        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n        librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n        plt.colorbar(format='%+2.0f dB')\n        plt.title(f\"Spectrogram - Sample {idx}\")\n    \n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()\n\nplot_spectrograms(train_df, \"Training Set Audio Examples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_mfccs(df, title, sr=16000):\n    plt.figure(figsize=(15, 5))\n    for i in range(3):\n        idx = np.random.choice(df.index)\n        audio_path = df.loc[idx, 'audio_path']\n        y, sr = librosa.load(audio_path, sr=sr)\n        \n        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n        \n        plt.subplot(1, 3, i+1)\n        librosa.display.specshow(mfccs, x_axis='time')\n        plt.colorbar()\n        plt.title(f\"MFCCs - Sample {idx}\")\n    \n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()\n\nplot_mfccs(train_df, \"Validation Set MFCC Examples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_pitch_distribution(df, title):\n    pitches = []\n    for _, row in df.sample(100).iterrows():  # Subsample for speed\n        y, sr = librosa.load(row['audio_path'], sr=None)  # Keep original SR\n        pitch = librosa.yin(y, fmin=50, fmax=2000, sr=sr)\n        valid_pitches = pitch[(pitch > 50) & (pitch < 2000)]  # Explicit valid range\n        pitches.extend(valid_pitches)\n    \n    plt.figure(figsize=(12, 6))\n    \n    # Use KDE for smoother distribution\n    sns.kdeplot(pitches, fill=True, log_scale=True)\n    \n    # Add musical reference lines\n    for note in [55, 110, 220, 440, 880]:  # A1, A2, A3, A4, A5\n        plt.axvline(note, color='red', linestyle='--', alpha=0.3)\n    \n    plt.title(f'{title} - Pitch Distribution')\n    plt.xlabel('Fundamental Frequency (Hz) [Log Scale]')\n    plt.ylabel('Density')\n    plt.xscale('log')\n    plt.xticks([50, 100, 200, 400, 800, 1600], \n               ['50', '100', '200', '400', '800', '1600'])\n    plt.grid(True, which='both', linestyle='--', alpha=0.5)\n    plt.show()\n\nplot_pitch_distribution(train_df, 'Training Set')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_silence_distribution(df, title):\n    silence_percentages = []\n    \n    for _, row in df.iterrows():\n        # Load audio with consistent sample rate (match your preprocessing SR)\n        y, sr = librosa.load(row['audio_path'], sr=16000)  # Match your preprocessing SR\n        \n        # Get NON-silent intervals (common confusion!)\n        non_silent_intervals = librosa.effects.split(\n            y, \n            top_db=30,  # Same threshold as preprocessing\n            frame_length=1024,\n            hop_length=256\n        )\n        # Calculate SILENT duration (total - non-silent)\n        total_duration = len(y) / sr\n        non_silent_duration = sum(end - start for (start, end) in non_silent_intervals) / sr\n        silent_duration = total_duration - non_silent_duration\n        \n        # Handle edge case: avoid division by zero\n        if total_duration > 0:\n            silence_percent = (silent_duration / total_duration) * 100\n            silence_percentages.append(silence_percent)\n        else:\n            silence_percentages.append(0)  # Handle 0-duration files\n    \n    # Create visualization\n    plt.figure(figsize=(12, 6))\n    \n    # Use KDE plot for better distribution understanding\n    ax = sns.histplot(\n        silence_percentages, \n        bins=30,\n        kde=True,\n        stat='percent',\n        color='skyblue',\n        edgecolor='black'\n    )\n    \n    # Add critical annotations\n    mean_val = np.mean(silence_percentages)\n    median_val = np.median(silence_percentages)\n    plt.axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.1f}%')\n    plt.axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.1f}%')\n    \n    plt.title(f'{title} - Silence Percentage Distribution\\n(top_db=30, SR=16kHz)')\n    plt.xlabel('Percentage of Silence in Audio')\n    plt.ylabel('Percentage of Samples')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\nplot_silence_distribution(train_df, 'Training Set')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_temporal_features(df, title):\n    idx = np.random.choice(df.index)\n    audio_path = df.loc[idx, 'audio_path']\n    y, sr = librosa.load(audio_path)\n    \n    plt.figure(figsize=(15, 8))\n    \n    # Waveform\n    plt.subplot(3, 1, 1)\n    librosa.display.waveshow(y, sr=sr)\n    plt.title(f'Waveform - {title}')\n    \n    # RMS Energy\n    plt.subplot(3, 1, 2)\n    S, phase = librosa.magphase(librosa.stft(y))\n    rms = librosa.feature.rms(S=S)\n    times = librosa.times_like(rms)\n    plt.plot(times, rms[0])\n    plt.title('RMS Energy')\n    \n    # Spectral Rolloff\n    plt.subplot(3, 1, 3)\n    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n    plt.plot(times, rolloff[0])\n    plt.title('Spectral Rolloff')\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_temporal_features(train_df, 'Training Sample Temporal Features')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_sample_fft(df, title):\n    idx = np.random.choice(df.index)\n    y, sr = librosa.load(df.loc[idx, 'audio_path'])\n    \n    # Compute FFT\n    fft = np.fft.fft(y)\n    magnitude = np.abs(fft)\n    frequency = np.linspace(0, sr, len(magnitude))\n    \n    plt.figure(figsize=(15, 5))\n    plt.plot(frequency[:len(frequency)//2], magnitude[:len(magnitude)//2])  # Only plot first half (Nyquist)\n    plt.title(f'{title} - FFT Analysis\\nSample {idx} (Duration: {df.loc[idx, \"duration\"]:.2f}s)')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Magnitude')\n    plt.grid()\n    plt.show()\n\nplot_sample_fft(train_df, 'Training Set')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_average_fft(df, title, n_samples=10):\n    plt.figure(figsize=(15, 5))\n    all_magnitudes = []\n    \n    for path in df['audio_path'].sample(n_samples):\n        y, sr = librosa.load(path)\n        fft = np.fft.fft(y)\n        magnitude = np.abs(fft)[:len(y)//2]  # Take first half\n        all_magnitudes.append(magnitude)\n    \n    # Compute mean spectrum\n    avg_magnitude = np.mean(all_magnitudes, axis=0)\n    frequency = np.linspace(0, sr//2, len(avg_magnitude))\n    \n    plt.plot(frequency, avg_magnitude)\n    plt.title(f'{title} - Average FFT Spectrum ({n_samples} samples)')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Average Magnitude')\n    plt.grid()\n    plt.show()\n\nplot_average_fft(train_df, 'Training Set')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_average_fft(df, title, n_samples=10):\n    plt.figure(figsize=(15, 5))\n    all_magnitudes = []\n    \n    for path in df['audio_path'].sample(n_samples):\n        y, sr = librosa.load(path)\n        fft = np.fft.fft(y)\n        magnitude = np.abs(fft)[:len(y)//2]  # Take first half\n        all_magnitudes.append(magnitude)\n    \n    # Compute mean spectrum\n    avg_magnitude = np.mean(all_magnitudes, axis=0)\n    frequency = np.linspace(0, sr//2, len(avg_magnitude))\n    \n    plt.plot(frequency, avg_magnitude)\n    plt.title(f'{title} - Average FFT Spectrum ({n_samples} samples)')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Average Magnitude')\n    plt.grid()\n    plt.show()\n\nplot_average_fft(train_df, 'Training Set')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_frequency_bands(df, title):\n    band_energies = {'low': [], 'mid': [], 'high': []}\n    \n    for path in df['audio_path'].sample(100):  # Subsample\n        y, sr = librosa.load(path)\n        fft = np.fft.fft(y)\n        magnitude = np.abs(fft)[:len(y)//2]\n        frequency = np.linspace(0, sr//2, len(magnitude))  # Match length\n\n        bands = {\n            'low': (0, 200),\n            'mid': (200, 2000),\n            'high': (2000, sr//2)\n        }\n\n        for band, (low, high) in bands.items():\n            idx = np.where((frequency >= low) & (frequency <= high))\n            band_energies[band].append(np.sum(magnitude[idx]))\n\n    plt.figure(figsize=(10, 5))\n    sns.boxplot(data=pd.DataFrame(band_energies))\n    plt.title(f'{title} - Frequency Band Energy Distribution')\n    plt.ylabel('Energy Sum')\n    plt.show()\n\n\nsample_path = train_df['audio_path'].iloc[0]\ny, sr = librosa.load(sample_path)\nfrequency = np.linspace(0, sr//2, len(y)//2)\nplot_frequency_bands(train_df, 'Training Set')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training ","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_pandas(train_df[['audio_path', 'text']])\ntest_dataset = Dataset.from_pandas(test_df[['audio_path', 'text']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom transformers import WhisperFeatureExtractor\n\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"vasista22/whisper-tamil-large-v2\")\nfrom transformers import WhisperTokenizer\n\ntokenizer = WhisperTokenizer.from_pretrained(\"vasista22/whisper-tamil-large-v2\", language=\"ta\", task=\"transcribe\")\nfrom transformers import WhisperProcessor\n\nprocessor = WhisperProcessor.from_pretrained(\"vasista22/whisper-tamil-large-v2\", language=\"ta\", task=\"transcribe\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa\n\ndef prepare_dataset(batch):\n    # Load audio and compute features\n    audio, sr = librosa.load(batch[\"audio_path\"], sr=16000)\n    batch[\"input_features\"] = processor(audio, sampling_rate=sr, return_tensors=\"pt\").input_features[0]\n    \n    # Tokenize text\n    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n    return batch\n\ntrain_dataset = train_dataset.map(prepare_dataset)\ntest_dataset = test_dataset.map(prepare_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport torch\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # Split inputs and labels\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        # Pad input features\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # Pad labels\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # Replace padding with -100 (to ignore in loss)\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n        \n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n        return batch\n\n# Initialize collator\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"wer\")\ncer_metric = evaluate.load(\"cer\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    if isinstance(pred_ids, tuple):\n        pred_ids = pred_ids[0]\n    \n    if len(pred_ids.shape) > 2:\n        pred_ids = pred_ids.argmax(axis=-1)\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n    cer = 100 * cer_metric.compute(predictions=pred_str, references=label_str)\n    \n    return {\"wer\": wer, \"cer\": cer}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import WhisperForConditionalGeneration\n\nmodel = WhisperForConditionalGeneration.from_pretrained(\"vasista22/whisper-tamil-large-v2\", \n                                                        load_in_8bit=True, \n                                                        device_map=\"auto\",\n                                                        use_safetensors=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.config.forced_decoder_ids = None\nmodel.config.suppress_tokens = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel = prepare_model_for_kbit_training(model)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n\nconfig = LoraConfig(r=64, \n                    lora_alpha=128,\n                    target_modules=[\"k_proj\",\"q_proj\", \"v_proj\",\"dense\"], \n                    lora_dropout=0.05, \n                    bias=\"none\",\n                    use_dora=True)\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"temp\",  \n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,  \n    learning_rate=1e-5,\n    warmup_steps=50,\n    num_train_epochs=3,\n    eval_strategy=\"epoch\",\n    fp16=True,\n    per_device_eval_batch_size=8,\n    generation_max_length=128,\n    logging_steps=25,\n    remove_unused_columns=False,\n    label_names=[\"labels\"],  \n    report_to=\"none\", \n    logging_dir=\"./logs\" \n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)\nmodel.config.use_cache = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prediction Generation","metadata":{}},{"cell_type":"code","source":"# First save your trained model and processor\ntrainer.model.save_pretrained(\"/kaggle/working/whisper-tamil-lora\")\nprocessor.save_pretrained(\"/kaggle/working/whisper-tamil-lora\")\n\n# Load the test samples\ntest_df = pd.read_csv('/kaggle/working/final_test_files.csv')\naudio_dir = '/kaggle/input/tamil-dataset-for-asr/Training/Audio'\ntext_dir = '/kaggle/input/tamil-dataset-for-asr/Training/Transcripts'\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\nbase_model = WhisperForConditionalGeneration.from_pretrained(\"vasista22/whisper-tamil-large-v2\",\n                                                            device_map=\"auto\",\n                                                            use_safetensors=True)\nmodel = PeftModel.from_pretrained(base_model, \"/kaggle/working/whisper-tamil-lora\")\nprocessor = WhisperProcessor.from_pretrained(\"/kaggle/working/whisper-tamil-lora\")\n\n# Move model to device and set to eval mode\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\nmodel.eval()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to get actual transcript\ndef get_actual_transcript(file_id):\n    text_path = os.path.join(text_dir, f\"{file_id}.txt\")\n    with open(text_path, \"r\", encoding=\"utf-8\") as f:\n        return f.read().strip()\n\n# Function to calculate WER and CER\ndef calculate_metrics(pred_text, actual_text):\n    return {\n        \"wer\": metric.compute(predictions=[pred_text], references=[actual_text]),\n        \"cer\": cer_metric.compute(predictions=[pred_text], references=[actual_text])\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tabulate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prediction and evaluation\nresults = []\n\nfor file_id in test_df['file_id']:\n    # Load audio\n    audio_path = os.path.join(audio_dir, f\"{file_id}.wav\")\n    audio, sr = librosa.load(audio_path, sr=16000)\n    \n    # Process audio\n    inputs = processor(\n        audio, \n        sampling_rate=16000, \n        return_tensors=\"pt\"\n    ).input_features.to(device)\n    \n    # Generate prediction\n    with torch.no_grad():\n        generated_ids = model.generate(inputs=inputs, max_length=448)\n    \n    # Decode prediction\n    pred_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    \n    # Get actual text\n    actual_text = get_actual_transcript(file_id)\n    \n    # Calculate metrics\n    metrics = calculate_metrics(pred_text, actual_text)\n    \n    print(\"Current file ID : \",file_id)\n    \n    results.append({\n        \"file_id\": file_id,\n        \"prediction\": pred_text,\n        \"actual\": actual_text,\n        \"WER\": f\"{metrics['wer']*100:.2f}%\",\n        \"CER\": f\"{metrics['cer']*100:.2f}%\"\n    })\n\n# Create and display results table\nresults_df = pd.DataFrame(results)\nprint(\"\\nEvaluation Results:\")\nprint(tabulate(results_df[[\"prediction\", \"actual\", \"WER\", \"CER\"]], \n             headers=[\"Prediction\", \"Actual\", \"WER\", \"CER\"], \n             tablefmt=\"pretty\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_df['actual'][1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_df['prediction'][1]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}